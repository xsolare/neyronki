<!--
 1. Задача классификации - распознавание меток классов
 2. Задача регрессии - предсказание значений непрерывной целевой переменной
 3. Обнаружение скрытых структур при помощи обучения без учителя
 4. Выявление подгрупп при помощи кластеризации
 5. Предобработка - приведение данных в приемлемый вид
 6. Тренировка и отбор прогнозной модели. Оценка моделей и прогнозирование на ранее не встречавшихся экземплярах данных.
 7. Искусственные нейроны - итория машинного обучения
 8. Адаптивные линейные нейроны и сходимость обучения
 9. Минимизация функций стоимости методом градиентного спуска
10. Моделирование вероятностей классов логистической регрессии
11. Решение проблемы переобучения при помощи регуляризации
12. Классификация с максимальным зазором на основе метода опорных векторов
13. Решение нелинейных задач ядерным методом SVM
14. Использование ядерного трюка для нахождения разделяющих гиперплоскостей в пространстве высокой размерности
15. Обучение моделей на основе деревьев решений
16. Объединение множества признаков принятия решений с помощью случайных лесов.
17. k ближайших соседей - алгоритм ленивого обучения
18. Приведение признаков к одинаковой шкале
19. Снижение размерности без учителя на основе анализа главных компонент
20. Сжатие данных с учителем путем линейного дискриминантного анализа
21. Реализация ядерного метода анализа главных компонент
22. Ядерный метод анализа главных компонент
23. Использование k-блочной перекрестной проверки для оценки качества модели
24. Решение проблемы переобучения и недообучения при помощи
25. Объединение моделей для методов ансамблевого обучения
26. Бэггинг - сборка ансамбля классификаторов из бутстрап-выборок
27. Прогнозирование значений непрерывной целевой переменной на основе регрессионного. анализа
28. Реализация линейной регрессионной модели методом наименьших квадратов
29. Подгонка стабильной регрессионной модели алгоритмом RANSAC
30. Оценивание качества работы линейных регрессионных моделей
31. Применение регуляризованных методов для регрессии
32. Превращение линейной регрессионной модели в криволинейную - полиномиальная регрессия
33. Работа с немаркированными данными - кластерный анализ
34. Группирование объектов по подобию методом k средних
35. Алгоритм k-средних
36. Использование метода локтя для нахождения оптимального числа кластеров
37. Количественная оценка качества кластеризации методом силуэтных графиков
38. Организация кластеров в виде иерархического дерева
39. Локализация областей высокой плотности алгоритмом DBSCAN
40. Активация нейронной сети методом прямого распространения сигналов
41. Тренировка нейронных сетей методом обратного распространения ошибки
42. Сверточные нейронные сети
43. Рекуррентные нейронные сети
-->

<div id="readme-top" style="display: flex; align-items: center; justify-content: space-between;">
  <h1>Содержание экзамена МИС</h1>
  <a href="https://github.com/xsolare/neyronki"><img src="https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white" /></a>
</div>

<ol>
  <li><a href="#1"> Задача классификации - распознавание меток классов </a></li>
  <li><a href="#2"> Задача регрессии - предсказание значений непрерывной целевой переменной </a></li>
  <li><a href="#3"> Обнаружение скрытых структур при помощи обучения без учителя </a></li>
  <li><a href="#4"> Выявление подгрупп при помощи кластеризации </a></li>
  <li><a href="#5"> Предобработка - приведение данных в приемлемый вид </a></li>
  <li><a href="#6"> Тренировка и отбор прогнозной модели. Оценка моделей и прогнозирование на ранее не встречавшихся экземплярах данных. </a></li>
  <li><a href="#7"> Искусственные нейроны - итория машинного обучения </a></li>
  <li><a href="#8"> Адаптивные линейные нейроны и сходимость обучения </a></li>
  <li><a href="#9"> Минимизация функций стоимости методом градиентного спуска </a></li>
  <li><a href="#10"> Моделирование вероятностей классов логистической регрессии </a></li>
  <li><a href="#11"> Решение проблемы переобучения при помощи регуляризации </a></li>
  <li><a href="#12"> Классификация с максимальным зазором на основе метода опорных векторов </a></li>
  <li><a href="#13"> Решение нелинейных задач ядерным методом SVM </a></li>
  <li><a href="#14"> Использование ядерного трюка для нахождения разделяющих гиперплоскостей в пространстве высокой размерности </a></li>
  <li><a href="#15"> Обучение моделей на основе деревьев решений </a></li>
  <li><a href="#16"> Объединение множества признаков принятия решений с помощью случайных лесов. </a></li>
  <li><a href="#17"> k ближайших соседей - алгоритм ленивого обучения </a></li>
  <li><a href="#18"> Приведение признаков к одинаковой шкале </a></li>
  <li><a href="#19"> Снижение размерности без учителя на основе анализа главных компонент </a></li>
  <li><a href="#20"> Сжатие данных с учителем путем линейного дискриминантного анализа </a></li>
  <li><a href="#21"> Реализация ядерного метода анализа главных компонент </a></li>
  <li><a href="#22"> Ядерный метод анализа главных компонент </a></li>
  <li><a href="#23"> Использование k-блочной перекрестной проверки для оценки качества модели </a></li>
  <li><a href="#24"> Решение проблемы переобучения и недообучения при помощи </a></li>
  <li><a href="#25"> Объединение моделей для методов ансамблевого обучения </a></li>
  <li><a href="#26"> Бэггинг - сборка ансамбля классификаторов из бутстрап-выборок </a></li>
  <li><a href="#27"> Прогнозирование значений непрерывной целевой переменной на основе регрессионного. анализа </a></li>
  <li><a href="#28"> Реализация линейной регрессионной модели методом наименьших квадратов </a></li>
  <li><a href="#29"> Подгонка стабильной регрессионной модели алгоритмом RANSAC </a></li>
  <li><a href="#30"> Оценивание качества работы линейных регрессионных моделей </a></li>
  <li><a href="#31"> Применение регуляризованных методов для регрессии </a></li>
  <li><a href="#32"> Превращение линейной регрессионной модели в криволинейную - полиномиальная регрессия </a></li>
  <li><a href="#33"> Работа с немаркированными данными - кластерный анализ </a></li>
  <li><a href="#34"> Группирование объектов по подобию методом k средних </a></li>
  <li><a href="#35"> Алгоритм k-средних </a></li>
  <li><a href="#36"> Использование метода локтя для нахождения оптимального числа кластеров </a></li>
  <li><a href="#37"> Количественная оценка качества кластеризации методом силуэтных графиков </a></li>
  <li><a href="#38"> Организация кластеров в виде иерархического дерева </a></li>
  <li><a href="#39"> Локализация областей высокой плотности алгоритмом DBSCAN </a></li>
  <li><a href="#40"> Активация нейронной сети методом прямого распространения сигналов </a></li>
  <li><a href="#41"> Тренировка нейронных сетей методом обратного распространения ошибки </a></li>
  <li><a href="#42"> Сверточные нейронные сети </a></li>
  <li><a href="#43"> Рекуррентные нейронные сети </a></li>
  <!-- <a href="#ggd">Если не знаете ответ на вопрос, то это должно вас выручить</a> -->
</ol>
<hr/>
<br />

##

<h2 id="1">  1. Задача классификации - распознавание меток классов </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="2">  2. Задача регрессии - предсказание значений непрерывной целевой переменной </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="3">  3. Обнаружение скрытых структур при помощи обучения без учителя </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="4">  4. Выявление подгрупп при помощи кластеризации </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="5">  5. Предобработка - приведение данных в приемлемый вид </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="6">  6. Тренировка и отбор прогнозной модели. Оценка моделей и прогнозирование на ранее не встречавшихся экземплярах данных. </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="7">  7. Искусственные нейроны - итория машинного обучения </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="8">  8. Адаптивные линейные нейроны и сходимость обучения </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="9">  9. Минимизация функций стоимости методом градиентного спуска </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="10">  10. Моделирование вероятностей классов логистической регрессии </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="11">  11. Решение проблемы переобучения при помощи регуляризации </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="12">  12. Классификация с максимальным зазором на основе метода опорных векторов </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="13">  13. Решение нелинейных задач ядерным методом SVM </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="14">  14. Использование ядерного трюка для нахождения разделяющих гиперплоскостей в пространстве высокой размерности </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="15">  15. Обучение моделей на основе деревьев решений </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="16">  16. Объединение множества признаков принятия решений с помощью случайных лесов. </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="17">  17. k ближайших соседей - алгоритм ленивого обучения </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="18">  18. Приведение признаков к одинаковой шкале </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="19">  19. Снижение размерности без учителя на основе анализа главных компонент </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="20">  20. Сжатие данных с учителем путем линейного дискриминантного анализа </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="21">  21. Реализация ядерного метода анализа главных компонент </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="22">  22. Ядерный метод анализа главных компонент </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="23">  23. Использование k-блочной перекрестной проверки для оценки качества модели </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="24">  24. Решение проблемы переобучения и недообучения при помощи </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="25">  25. Объединение моделей для методов ансамблевого обучения </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="26">  26. Бэггинг - сборка ансамбля классификаторов из бутстрап-выборок </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="27">  27. Прогнозирование значений непрерывной целевой переменной на основе регрессионного. анализа </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="28">  28. Реализация линейной регрессионной модели методом наименьших квадратов </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="29">  29. Подгонка стабильной регрессионной модели алгоритмом RANSAC </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="30">  30. Оценивание качества работы линейных регрессионных моделей </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="31">  31. Применение регуляризованных методов для регрессии </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="32">  32. Превращение линейной регрессионной модели в криволинейную - полиномиальная регрессия </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="33">  33. Работа с немаркированными данными - кластерный анализ </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="34">  34. Группирование объектов по подобию методом k средних </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="35">  35. Алгоритм k-средних </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="36">  36. Использование метода локтя для нахождения оптимального числа кластеров </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="37">  37. Количественная оценка качества кластеризации методом силуэтных графиков </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="38">  38. Организация кластеров в виде иерархического дерева </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="39">  39. Локализация областей высокой плотности алгоритмом DBSCAN </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="40">  40. Активация нейронной сети методом прямого распространения сигналов </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="41">  41. Тренировка нейронных сетей методом обратного распространения ошибки </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>

<hr/>

##

<h2 id="42">  42. Сверточные нейронные сети </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>

<hr/>

##

<h2 id="43">  43. Рекуррентные нейронные сети </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
