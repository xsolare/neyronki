<!--
 1. Задача классификации - распознавание меток классов
 2. Задача регрессии - предсказание значений непрерывной целевой переменной
 3. Обнаружение скрытых структур при помощи обучения без учителя
 4. Выявление подгрупп при помощи кластеризации
 5. Предобработка - приведение данных в приемлемый вид
 6. Тренировка и отбор прогнозной модели. Оценка моделей и прогнозирование на ранее не встречавшихся экземплярах данных.
 7. Искусственные нейроны - итория машинного обучения
 8. Адаптивные линейные нейроны и сходимость обучения
 9. Минимизация функций стоимости методом градиентного спуска
10. Моделирование вероятностей классов логистической регрессии
11. Решение проблемы переобучения при помощи регуляризации
12. Классификация с максимальным зазором на основе метода опорных векторов
13. Решение нелинейных задач ядерным методом SVM
14. Использование ядерного трюка для нахождения разделяющих гиперплоскостей в пространстве высокой размерности
15. Обучение моделей на основе деревьев решений
16. Объединение множества признаков принятия решений с помощью случайных лесов.
17. k ближайших соседей - алгоритм ленивого обучения
18. Приведение признаков к одинаковой шкале
19. Снижение размерности без учителя на основе анализа главных компонент
20. Сжатие данных с учителем путем линейного дискриминантного анализа
21. Реализация ядерного метода анализа главных компонент
22. Ядерный метод анализа главных компонент
23. Использование k-блочной перекрестной проверки для оценки качества модели
24. Решение проблемы переобучения и недообучения при помощи
25. Объединение моделей для методов ансамблевого обучения
26. Бэггинг - сборка ансамбля классификаторов из бутстрап-выборок
27. Прогнозирование значений непрерывной целевой переменной на основе регрессионного. анализа
28. Реализация линейной регрессионной модели методом наименьших квадратов
29. Подгонка стабильной регрессионной модели алгоритмом RANSAC
30. Оценивание качества работы линейных регрессионных моделей
31. Применение регуляризованных методов для регрессии
32. Превращение линейной регрессионной модели в криволинейную - полиномиальная регрессия
33. Работа с немаркированными данными - кластерный анализ
34. Группирование объектов по подобию методом k средних
35. Алгоритм k-средних
36. Использование метода локтя для нахождения оптимального числа кластеров
37. Количественная оценка качества кластеризации методом силуэтных графиков
38. Организация кластеров в виде иерархического дерева
39. Локализация областей высокой плотности алгоритмом DBSCAN
40. Активация нейронной сети методом прямого распространения сигналов
41. Тренировка нейронных сетей методом обратного распространения ошибки
42. Сверточные нейронные сети
43. Рекуррентные нейронные сети
-->

<div id="readme-top" style="display: flex; align-items: center; justify-content: space-between;">
  <h1>Содержание экзамена МИС</h1>
  <a href="https://github.com/xsolare/neyronki"><img src="https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white" /></a>
</div>

<ol>
  <li>❌ <a href="#1"> Задача классификации - распознавание меток классов </a></li>
  <li>❌ <a href="#2"> Задача регрессии - предсказание значений непрерывной целевой переменной </a></li>
  <li>❌ <a href="#3"> Обнаружение скрытых структур при помощи обучения без учителя </a></li>
  <li>❌ <a href="#4"> Выявление подгрупп при помощи кластеризации </a></li>
  <li>❌ <a href="#5"> Предобработка - приведение данных в приемлемый вид </a></li>
  <li>❌ <a href="#6"> Тренировка и отбор прогнозной модели. Оценка моделей и прогнозирование на ранее не встречавшихся экземплярах данных. </a></li>
  <li>❌ <a href="#7"> Искусственные нейроны - итория машинного обучения </a></li>
  <li>❌ <a href="#8"> Адаптивные линейные нейроны и сходимость обучения </a></li>
  <li>❌ <a href="#9"> Минимизация функций стоимости методом градиентного спуска </a></li>
  <li>❌ <a href="#10"> Моделирование вероятностей классов логистической регрессии </a></li>
  <li>❌ <a href="#11"> Решение проблемы переобучения при помощи регуляризации </a></li>
  <li>❌ <a href="#12"> Классификация с максимальным зазором на основе метода опорных векторов </a></li>
  <li>❌ <a href="#13"> Решение нелинейных задач ядерным методом SVM </a></li>
  <li>❌ <a href="#14"> Использование ядерного трюка для нахождения разделяющих гиперплоскостей в пространстве высокой размерности </a></li>
  <li>❌ <a href="#15"> Обучение моделей на основе деревьев решений </a></li>
  <li>❌ <a href="#16"> Объединение множества признаков принятия решений с помощью случайных лесов. </a></li>
  <li>❌ <a href="#17"> k ближайших соседей - алгоритм ленивого обучения </a></li>
  <li>❌ <a href="#18"> Приведение признаков к одинаковой шкале </a></li>
  <li>❌ <a href="#19"> Снижение размерности без учителя на основе анализа главных компонент </a></li>
  <li>❌ <a href="#20"> Сжатие данных с учителем путем линейного дискриминантного анализа </a></li>
  <li>❌ <a href="#21"> Реализация ядерного метода анализа главных компонент </a></li>
  <li>❌ <a href="#22"> Ядерный метод анализа главных компонент </a></li>
  <li>❌ <a href="#23"> Использование k-блочной перекрестной проверки для оценки качества модели </a></li>
  <li>❌ <a href="#24"> Решение проблемы переобучения и недообучения при помощи </a></li>
  <li>❌ <a href="#25"> Объединение моделей для методов ансамблевого обучения </a></li>
  <li>❌ <a href="#26"> Бэггинг - сборка ансамбля классификаторов из бутстрап-выборок </a></li>
  <li>❌ <a href="#27"> Прогнозирование значений непрерывной целевой переменной на основе регрессионного. анализа </a></li>
  <li>❌ <a href="#28"> Реализация линейной регрессионной модели методом наименьших квадратов </a></li>
  <li>❌ <a href="#29"> Подгонка стабильной регрессионной модели алгоритмом RANSAC </a></li>
  <li>❌ <a href="#30"> Оценивание качества работы линейных регрессионных моделей </a></li>
  <li>❌ <a href="#31"> Применение регуляризованных методов для регрессии </a></li>
  <li>❌ <a href="#32"> Превращение линейной регрессионной модели в криволинейную - полиномиальная регрессия </a></li>
  <li>❌ <a href="#33"> Работа с немаркированными данными - кластерный анализ </a></li>
  <li>❌ <a href="#34"> Группирование объектов по подобию методом k средних </a></li>
  <li>❌ <a href="#35"> Алгоритм k-средних </a></li>
  <li>❌ <a href="#36"> Использование метода локтя для нахождения оптимального числа кластеров </a></li>
  <li>❌ <a href="#37"> Количественная оценка качества кластеризации методом силуэтных графиков </a></li>
  <li>✅ <a href="#38"> Организация кластеров в виде иерархического дерева </a></li>
  <li>❌ <a href="#39"> Локализация областей высокой плотности алгоритмом DBSCAN </a></li>
  <li>❌ <a href="#40"> Активация нейронной сети методом прямого распространения сигналов </a></li>
  <li>✅ <a href="#41"> Тренировка нейронных сетей методом обратного распространения ошибки </a></li>
  <li>✅ <a href="#42"> Сверточные нейронные сети </a></li>
  <li>✅ <a href="#43"> Рекуррентные нейронные сети </a></li>
  <!-- <a href="#ggd">Если не знаете ответ на вопрос, то это должно вас выручить</a> -->
</ol>
<hr/>
<br />

##

<h2 id="1">  1. Задача классификации - распознавание меток классов </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="2">  2. Задача регрессии - предсказание значений непрерывной целевой переменной </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="3">  3. Обнаружение скрытых структур при помощи обучения без учителя </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="4">  4. Выявление подгрупп при помощи кластеризации </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="5">  5. Предобработка - приведение данных в приемлемый вид </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="6">  6. Тренировка и отбор прогнозной модели. Оценка моделей и прогнозирование на ранее не встречавшихся экземплярах данных. </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="7">  7. Искусственные нейроны - итория машинного обучения </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="8">  8. Адаптивные линейные нейроны и сходимость обучения </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="9">  9. Минимизация функций стоимости методом градиентного спуска </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="10">  10. Моделирование вероятностей классов логистической регрессии </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="11">  11. Решение проблемы переобучения при помощи регуляризации </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="12">  12. Классификация с максимальным зазором на основе метода опорных векторов </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="13">  13. Решение нелинейных задач ядерным методом SVM </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="14">  14. Использование ядерного трюка для нахождения разделяющих гиперплоскостей в пространстве высокой размерности </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="15">  15. Обучение моделей на основе деревьев решений </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="16">  16. Объединение множества признаков принятия решений с помощью случайных лесов. </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="17">  17. k ближайших соседей - алгоритм ленивого обучения </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="18">  18. Приведение признаков к одинаковой шкале </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="19">  19. Снижение размерности без учителя на основе анализа главных компонент </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="20">  20. Сжатие данных с учителем путем линейного дискриминантного анализа </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="21">  21. Реализация ядерного метода анализа главных компонент </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="22">  22. Ядерный метод анализа главных компонент </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="23">  23. Использование k-блочной перекрестной проверки для оценки качества модели </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="24">  24. Решение проблемы переобучения и недообучения при помощи </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="25">  25. Объединение моделей для методов ансамблевого обучения </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="26">  26. Бэггинг - сборка ансамбля классификаторов из бутстрап-выборок </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="27">  27. Прогнозирование значений непрерывной целевой переменной на основе регрессионного. анализа </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="28">  28. Реализация линейной регрессионной модели методом наименьших квадратов </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="29">  29. Подгонка стабильной регрессионной модели алгоритмом RANSAC </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="30">  30. Оценивание качества работы линейных регрессионных моделей </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="31">  31. Применение регуляризованных методов для регрессии </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="32">  32. Превращение линейной регрессионной модели в криволинейную - полиномиальная регрессия </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="33">  33. Работа с немаркированными данными - кластерный анализ </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="34">  34. Группирование объектов по подобию методом k средних </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="35">  35. Алгоритм k-средних </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="36">  36. Использование метода локтя для нахождения оптимального числа кластеров </h2>

Метод локтя – один из самых известных методов, с помощью которого вы можете выбрать правильное значение k и повысить производительность Модели (Model). Этот эмпирический метод вычисляет сумму квадратов расстояний между точками и вычисляет Среднее значение (Mean).

<b>Пример</b>. Предположим, мы пошли в магазин за овощами и увидели, что они будут расположены на полках по типу. Вся морковь хранится в одном месте, картошка – в другом.

<img src="./assets/36/36-1.png">
<br/>
<br/>

До применения кластеризации (появления окрашенных зон и обозначения записей разными иконками) перепутать категорию довольно легко. Неопытные мерчендайзеры до сих пор кладут арбузы в отдел ягод, хоть и правы с научной точки зрения.

Метод k-средних пытается сгруппировать похожие элементы в три этапа:

Выберем значение k

1. Инициализируем центроиды (разделительные линии)
2. Выберем группу и найдем среднее значение расстояния между точками.
3. Давайте разберемся в вышеуказанных шагах с помощью иллюстраций.

Допустим, мы на глаз кластеризовали наблюдения, причислив половину к белой категории, оставшуюся часть – к розовой.

Шаг 1. Мы случайным образом выбираем значение K, равное 2:

<img src="./assets/36/36-2.png">
<br/>
<br/>
Существуют различные методы, с помощью которых мы можем выбрать правильные значения параметра k. Об этом позже.
<br/>
<br/>
Шаг 2. Соединим две выбранные максимально удаленные точки, обозначенные белой полупрозрачной обводкой. Теперь, чтобы определить центроид, мы построим перпендикуляр к этой линии:
<br/><br/>
<img src='./assets/36/36-3.png'>
<br/>
<br/>
Если вы заметили, одна белая точка попала в группу розовых, и теперь относится к другой группе, чем предположено изначально.
<br/>
<br/>
Шаг 3. Мы соединим две другие удаленные точки, проведем к ним перпендикулярную линию и найдем центроид. Теперь некоторые белые точки преобразуются в розовые:
<br/><br/>
<img src='./assets/36/36-4.png'>
<br/>
<br/>
Этот процесс будет продолжаться до тех пор, пока мы не переберем все возможные сочетания пар дистанцированных точек и не уточним границы кластеров. Стабильность центроидов определяется путем сравнения абсолютного значения изменения среднего Евклидова расстояния (Euclidian Distance) между наблюдениями и их соответствующими центроидами с пороговым значением.
<br/>
<br/>
<img src='./assets/36/36-5.png'>
<br/>
<br/>
Рассмотрим "локтевой" способ. Когда значение k равно 1, сумма квадрата внутри кластера будет большой. По мере увеличения значения k сумма квадратов расстояний внутри кластера будет уменьшаться.
<br/>
<br/>
Наконец, мы построим график между значениями k и суммой квадрата внутри кластера, чтобы получить значение k. Мы внимательно рассмотрим график. В какой-то момент значение по оси x резко уменьшится. Эта точка будет считаться оптимальным значением k: <br/>
<br/>
<img src="./assets/36/36-6.png">
<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="37">  37. Количественная оценка качества кластеризации методом силуэтных графиков </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="38">  38. Организация кластеров в виде иерархического дерева </h2>

<i>Тут даже пример кода есть:</i>
<a href="https://neerc.ifmo.ru/wiki/index.php?title=%D0%98%D0%B5%D1%80%D0%B0%D1%80%D1%85%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D0%BA%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F">source</a>

Иерархическая кластеризация — множество алгоритмов кластеризации, направленных на создание иерархии вложенных разбиений исходного множества объектов.

Иерархические алгоритмы кластеризации часто называют <b>алгоритмами таксономии</b>. Для визуального представления результатов кластеризации используется дендрограмма — дерево, построенное по матрице мер близости между кластерами. В узлах дерева находятся подмножества объектов из обучающей выборки. При этом на каждом ярусе дерева множество объектов из всех узлов составляет исходное множество объектов. Объединение узлов между ярусами соответствует слиянию двух кластеров. При этом длина ребра соответствует расстоянию между кластерами.

<h4>Алгоритм иерархической кластеризации</h4>

Дерево строится от листьев к корню. В начальный момент времени каждый объект содержится в собственном кластере. Далее происходит итеративный процесс слияния двух ближайших кластеров до тех пор, пока все кластеры не объединятся в один или не будет найдено необходимое число кластеров. На каждом шаге необходимо уметь вычислять расстояние между кластерами и пересчитывать расстояние между новыми кластерами. Расстояние между одноэлементными кластерами определяется через расстояние между объектами. Для вычисления расстояния между кластерами на практике используются различные функции в зависимости от специфики задачи.

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="39">  39. Локализация областей высокой плотности алгоритмом DBSCAN </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="40">  40. Активация нейронной сети методом прямого распространения сигналов </h2>

<p align="right"><a href="#readme-top">К содержанию</a></p>
<hr/>

##

<h2 id="41">  41. Тренировка нейронных сетей методом обратного распространения ошибки </h2>

<i>лучше прочитать вот тута:</i>
<a href="https://habr.com/ru/post/198268/">source</a>

Обучение сети методом обратного распространения ошибки включает в себя три этапа:

1. подачу на вход данных, с последующим распространением данных в направлении выходов
2. вычисление и обратное распространение соответствующей ошибки
3. корректировка весов.

После обучения предполагается лишь подача на вход сети данных и распространение их в направлении выходов. При этом, если обучение сети может являться довольно длительным процессом, то непосредственное вычисление результатов обученной сетью происходит очень быстро. Кроме того, существуют многочисленные вариации метода обратного распространения ошибки, разработанные с целью увеличения скорости протекания процесса обучения.

Также стоит отметить, что однослойная нейронная сеть существенно ограничена в том, обучению каким шаблонам входных данных она подлежит, в то время, как многослойная сеть ( с одним или более скрытым слоем ) не имеет такого недостатка. Далее будет дано описание стандартной нейронной сети с обратным распространением ошибки.

<b>ДАЛЬШЕ ВООБЩЕ ПИЗДЕЦ СОЧУВСТВУЮ</b>

Алгоритм, представленный далее, применим к нейронной сети с одним скрытым слоем, что является допустимой и адекватной ситуацией для большинства приложений.

Как уже было сказано ранее, обучение сети включает в себя три стадии:

- подача на входы сети обучающих данных
- обратное распространение ошибки
- корректировка весов.

1. В ходе первого этапа каждый входной нейрон получает сигнал и широковещательно транслирует его каждому из скрытых нейронов. Каждый скрытый нейрон затем вычисляет результат его активационной функции ( сетевой функции ) и рассылает свой сигнал всем выходным нейронам. Каждый выходной нейрон, в свою очередь, вычисляет результат своей активационной функции, который представляет собой ничто иное, как выходной сигнал данного нейрона для соответствующих входных данных. В процессе обучения, каждый нейрон на выходе сети сравнивает вычисленное значение с предоставленным учителем ( целевым значением ), определяя соответствующее значение ошибки для данного входного шаблона.
2. На основании этой ошибки вычисляется <u>Составляющая корректировки весов связей</u>, которая используется при распространении ошибки до всех элементов сети предыдущего слоя ( скрытых нейронов), а также позже при изменении весов связей между выходными нейронами и скрытыми. Аналогичным образом вычисляется для каждого скрытого нейрона. Несмотря на то, что распространять ошибку до входного слоя необходимости нет, используется для изменения весов связей между нейронами скрытого слоя и входными нейронами.
3. После того как все были определены, происходит одновременная корректировка весов всех связей.

<h4>Недостатки алгоритма</h4>
Несмотря на многочисленные успешные применения обратного распространения, оно не является универсальным решением. Больше всего неприятностей приносит неопределённо долгий процесс обучения. В сложных задачах для обучения сети могут потребоваться дни или даже недели, она может и вообще не обучиться. Причиной может быть одна из описанных ниже.

<h5>Паралич сети</h5>

В процессе обучения сети значения весов могут в результате коррекции стать очень большими величинами. Это может привести к тому, что все или большинство нейронов будут функционировать при очень больших значениях OUT, в области, где производная сжимающей функции очень мала. Так как посылаемая обратно в процессе обучения ошибка пропорциональна этой производной, то процесс обучения может практически замереть. В теоретическом отношении эта проблема плохо изучена. Обычно этого избегают уменьшением размера шага η, но это увеличивает время обучения. Различные эвристики использовались для предохранения от паралича или для восстановления после него, но пока что они могут рассматриваться лишь как экспериментальные.

<h5>Локальные минимумы</h5>

Обратное распространение использует разновидность градиентного спуска, то есть осуществляет спуск вниз по поверхности ошибки, непрерывно подстраивая веса в направлении к минимуму. Поверхность ошибки сложной сети сильно изрезана и состоит из холмов, долин, складок и оврагов в пространстве высокой размерности. Сеть может попасть в локальный минимум (неглубокую долину), когда рядом имеется гораздо более глубокий минимум. В точке локального минимума все направления ведут вверх, и сеть не способна из него выбраться. Основную трудность при обучении нейронных сетей составляют как раз методы выхода из локальных минимумов: каждый раз выходя из локального минимума снова ищется следующий локальный минимум тем же методом обратного распространения ошибки до тех пор, пока найти из него выход уже не удаётся.

<h5>Размер шага</h5>

Если размер шага фиксирован и очень мал, то сходимость слишком медленная, если же он фиксирован и слишком велик, то может возникнуть паралич или постоянная неустойчивость. Эффективно увеличивать шаг до тех пор, пока не прекратится улучшение оценки в данном направлении антиградиента и уменьшать, если такого улучшения не происходит.

<p align="right"><a href="#readme-top">К содержанию</a></p>

<hr/>

##

<h2 id="42">  42. Сверточные нейронные сети </h2>

<b>Свёрточная нейронная сеть</b> (СНС) — специальная архитектура искусственных нейронных сетей, нацеленная на эффективное распознавание образов, входит в состав технологий глубокого обучения.

<h4>Структура сверточной нейронной сети</h4>

В обычном перцептроне, который представляет собой полносвязную нейронную сеть, каждый нейрон связан со всеми нейронами предыдущего слоя, причём каждая связь имеет свой персональный весовой коэффициент. В свёрточной нейронной сети в <u>операции свёртки</u> используется лишь ограниченная матрица весов небольшого размера, которую «двигают» по всему обрабатываемому слою (в самом начале — непосредственно по входному изображению), формируя после каждого сдвига сигнал активации для нейрона следующего слоя с аналогичной позицией. То есть для различных нейронов выходного слоя используются одна и та же матрица весов, которую также называют <u>ядром свёртки</u>. Её интерпретируют как графическое кодирование какого-либо признака, например, наличие наклонной линии под определённым углом. Тогда следующий слой, получившийся в результате операции свёртки такой матрицей весов, показывает наличие данного признака в обрабатываемом слое и её координаты, формируя так называемую <u>карту признаков</u> (англ. feature map). Естественно, в свёрточной нейронной сети набор весов не один, а целая гамма, кодирующая элементы изображения (например линии и дуги под разными углами). При этом такие ядра свёртки не закладываются исследователем заранее, а формируются самостоятельно путём обучения сети классическим <u>методом обратного распространения ошибки</u>. Проход каждым набором весов формирует свой собственный экземпляр карты признаков, делая нейронную сеть многоканальной (много независимых карт признаков на одном слое). Также следует отметить, что при переборе слоя матрицей весов её передвигают обычно не на полный шаг (размер этой матрицы), а на небольшое расстояние. Так, например, при размерности матрицы весов 5×5 её сдвигают на один или два нейрона (пикселя) вместо пяти, чтобы не «перешагнуть» искомый признак.

Операция субдискретизации (операция подвыборки), выполняет уменьшение размерности сформированных карт признаков. В данной архитектуре сети считается, что информация о факте наличия искомого признака важнее точного знания его координат, поэтому из нескольких соседних нейронов карты признаков выбирается максимальный и принимается за один нейрон уплотнённой карты признаков меньшей размерности. <b>За счёт данной операции, помимо ускорения дальнейших вычислений, сеть становится более инвариантной к масштабу входного изображения.</b>

Рассмотрим типовую структуру свёрточной нейронной сети более подробно.

- Сеть состоит из большого количества слоёв.
- После начального слоя (входного изображения) сигнал проходит серию свёрточных слоёв, в которых чередуется собственно свёртка и субдискретизация (пулинг).
- Чередование слоёв позволяет составлять «карты признаков» из карт признаков, на каждом следующем слое карта уменьшается в размере, но увеличивается количество каналов. <br/>
  На практике это означает способность распознавания сложных иерархий признаков. Обычно после прохождения нескольких слоёв карта признаков вырождается в вектор или даже скаляр, но таких карт признаков становятся сотни. На выходе свёрточных слоёв сети дополнительно устанавливают несколько слоёв полносвязной нейронной сети (перцептрон), на вход которому подаются оконечные карты признаков.

<h4>1. Слой свёртки</h4>

Слой свёртки (англ. convolutional layer) — это основной блок свёрточной нейронной сети. Слой свёртки включает в себя для каждого канала свой фильтр, ядро свёртки которого обрабатывает предыдущий слой по фрагментам (суммируя результаты поэлементного произведения для каждого фрагмента). Весовые коэффициенты ядра свёртки (небольшой матрицы) неизвестны и устанавливаются в процессе обучения.

Особенностью свёрточного слоя является сравнительно небольшое количество параметров, устанавливаемое при обучении. Так например, если исходное изображение имеет размерность 100×100 пикселей по трём каналам (это значит 30 000 входных нейронов), а свёрточный слой использует фильтры c ядром 3×3 пикселя с выходом на 6 каналов, тогда в процессе обучения определяется только 9 весов ядра, однако по всем сочетаниям каналов, то есть 9×3×6=162, в таком случае данный слой требует нахождения только 162 параметров, что существенно меньше количества искомых параметров полносвязной нейронной сети.

<h4>2. Слой активации</h4>

Скалярный результат каждой свёртки попадает на <u>функцию активации</u>(определяет выходной сигнал, который определяется входным сигналом или набором входных сигналов), которая представляет собой некую нелинейную функцию. Слой активации обычно логически объединяют со слоем свёртки (считают, что функция активации встроена в слой свёртки).
То есть по сути это операция отсечения отрицательной части скалярной величины (?)

<h4>3. Пулинг или слой субдискретизации</h4>

Слой пулинга (иначе подвыборки, субдискретизации) представляет собой нелинейное уплотнение карты признаков, при этом группа пикселей (обычно размера 2×2) уплотняется до одного пикселя, проходя нелинейное преобразование. Преобразования затрагивают непересекающиеся прямоугольники или квадраты, каждый из которых ужимается в один пиксель, при этом выбирается пиксель, имеющий максимальное значение. <u>Операция пулинга позволяет существенно уменьшить пространственный объём изображения</u>.  
<b>Пулинг интерпретируется так:</b> если на предыдущей операции свёртки уже были выявлены некоторые признаки, то для дальнейшей обработки настолько подробное изображение уже не нужно, и оно уплотняется до менее подробного. К тому же фильтрация уже ненужных деталей помогает не переобучаться. Слой пулинга, как правило, вставляется после слоя свёртки перед слоем следующей свёртки.

<h4>4. Полносвязная нейронная сеть</h4>

После нескольких прохождений свёртки изображения и уплотнения с помощью пулинга система перестраивается от конкретной сетки пикселей с высоким разрешением к более абстрактным картам признаков, как правило, на каждом следующем слое увеличивается число каналов и уменьшается размерность изображения в каждом канале. В конце концов, остаётся большой набор каналов, хранящих небольшое число данных (даже один параметр), которые интерпретируются как самые абстрактные понятия, выявленные из исходного изображения.

Эти данные объединяются и передаются на обычную полносвязную нейронную сеть, которая тоже может состоять из нескольких слоёв. При этом полносвязные слои уже утрачивают пространственную структуру пикселей и обладают сравнительно небольшой размерностью (по отношению к количеству пикселей исходного изображения).

<p align="right"><a href="#readme-top">К содержанию</a></p>

<hr/>

##

<h2 id="43">  43. Рекуррентные нейронные сети </h2>

Рекуррентная нейронная сеть (англ. recurrent neural network, RNN) — вид нейронных сетей, где связи между элементами образуют направленную последовательность.

<h4>Области и примеры применения:</h4>
  Используются, когда важно соблюдать последовательность, когда важен порядок поступающих объектов.

<h5>1. Обработка текста на естественном языке:</h5>

- Анализ текста;
- Автоматический перевод;
<h5>2. Обработка аудио:</h5>

- Автоматическое распознавание речи;
<h5>3. Обработка видео:</h5>

- Прогнозирование следующего кадра на основе предыдущих;
- Распознавание эмоций;
<h5>4. Обработка изображений:</h5>

- Прогнозирование следующего пикселя на основе окружения;
- Генерация описания изображений.

<h4>Виды RNN:</h4>

1. <img src="./assets/43/43-1.png">
   <br/>
   <br/>

2. <img src="./assets/43/43-2.png">
   <br/>
   <br/>

3. <img src="./assets/43/43-3.png">
   <br/>
   <br/>

4. <img src="./assets/43/43-4.png">
   <br/>
   <br/>

<h4>Архитектуры</h4>

- <h5>Полностью рекуррентная сеть</h5>
  Это базовая архитектура, разработанная в 1980-х. Сеть строится из узлов, каждый из которых соединён со всеми другими узлами. У каждого нейрона порог активации меняется со временем и является вещественным числом. Каждое соединение имеет переменный вещественный вес. Узлы разделяются на входные, выходные и скрытые.

- <h5>Рекурсивная сеть</h5>
  Рекурсивные нейронные сети (англ. Recurrent neural networks) представляют собой более общий случай рекуррентных сетей, когда сигнал в сети проходит через структуру в виде дерева (обычно бинарные деревья). Те же самые матрицы весов используются рекурсивно по всему графу в соответствии с его топологией.

- <h5>Нейронная сеть Хопфилда</h5>
  Тип рекуррентной сети, когда все соединения симметричны. Изобретена Джоном Хопфилдом в 1982 году и гарантируется, что динамика такой сети сходится к одному из положений равновесия.

- <h5>Двунаправленная ассоциативная память (BAM)</h5>
  Вариацией сети Хопфилда является двунаправленная ассоциативная память (BAM). BAM имеет два слоя, каждый из которых может выступать в качестве входного, находить (вспоминать) ассоциацию и генерировать результат для другого слоя.

- <h5>Сети долго-краткосрочной памяти</h5>
  Сеть долго-краткосрочной памяти (англ. Long short-term memory, LSTM :)) является самой популярной архитектурой рекуррентной нейронной сети на текущий момент, такая архитектура способна запоминать данные на долгое время.

<i>За полным списокм и примером кода на питоне вам сюда:</i>
<a href="https://neerc.ifmo.ru/wiki/index.php?title=%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B5_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8">Source</a>

<p align="right"><a href="#readme-top">К содержанию</a></p>
